{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio based human profiling\n",
    "---\n",
    "> This project is for profiling humans with audio based technologies. Especially, it mines and describes speech microfeatures, and explores their link to myriad of human descriptors / profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~11/20/2017\n",
    "\n",
    "### Literature review\n",
    "Review literature regarding speech features and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~11/27/2017\n",
    "\n",
    "### Speech features\n",
    "\n",
    "Need\n",
    "\n",
    "1. large between-speaker variability and small within-speaker variability\n",
    "2. robust against noise and distortion\n",
    "3. obtainable\n",
    "4. frequent\n",
    "5. easy to measure\n",
    "6. difficult to mimic\n",
    "7. not affected by speaker's health or long-term variations in voice\n",
    "8. low dimensional\n",
    "\n",
    "Classification\n",
    "\n",
    "1. High-level: conversational, word & sentence level\n",
    "    1. Phones\n",
    "    2. Idiolect\n",
    "    3. Personal lexicon\n",
    "    4. Semantics\n",
    "    5. Accent\n",
    "    6. Pronunciation\n",
    "2. Mid-level: prosodic & spectro-temporal, tens or hundreds of ms, voice source / glottal flow related\n",
    "    1. Pitch\n",
    "    2. Energy\n",
    "    3. Duration\n",
    "    4. Rhythm\n",
    "    5. Temporal features: pitch onset and offset\n",
    "3. Low-level: spectral, short-term, frame level, 20-30ms\n",
    "    1. Spectrum: harmonics, cepstral, LPCs\n",
    "    2. Glottal pulse features\n",
    "4. Micro-level: transient, 1-20ms, fine-grained, spreading over entire signal\n",
    "    1. Aphonicity: unvoiced sound\n",
    "    2. Biphonicity: mix of two sounds\n",
    "    3. Breathiness: intra and between speech\n",
    "    4. Creakiness: harsh, high-pitched\n",
    "    5. Glottalization: complete or partial closure of the glottis during the articulation of another sound. e.g., glottal 'T'\n",
    "    6. Hoarseness: dysphonia. the acoustic properties of hoarseness are mainly determined by the interactions of the following three factors: 1) noise components in the main formant of each vowel, 2) high frequency noise components above 3 KHz, and 3) the loss of high frequency harmonic components.\n",
    "    7. Nasality\n",
    "    8. Jitter: in frequency\n",
    "    9. Vocal fry\n",
    "    10. Resonant\n",
    "    11. Shimmer: in amplitude\n",
    "\n",
    "    ...\n",
    "5. Latent: hidden, confounding, different domain\n",
    "    1. GMM stat\n",
    "    2. I-Vector\n",
    "    3. Net bottleneck\n",
    "    4. Z-features via adversarial latent matching\n",
    "\n",
    "Remarks\n",
    "\n",
    "1. Higher-level features are behavioral: learned, related to social status, education, origin, upbringing, etc.\n",
    "Lower-level features are organic: vocal folds, vocal tract, etc.\n",
    "\n",
    "2. High and mid-level features are more robust, but less discriminative, easier to mimic, difficult to extract, need more training data.\n",
    "Low-level features are easy to extract, need less training data, but more affected by noise and mismatch.\n",
    "\n",
    "3. Currently the literature often uses combination/fusion of the high, mid, low-level features.\n",
    "\n",
    "4. Micro-level features are difficult to measure.\n",
    "\n",
    "5. Latent features have confounding dimensions, and are hard to link to high, mid, low-level features.\n",
    "\n",
    "### Re-visit feature discovery via network dissection\n",
    "\n",
    "1. Last time: failed may be due to too short segments.\n",
    "This time: use longer speech segments.\n",
    "\n",
    "2. On classification tasks.\n",
    "\n",
    "3. First step: mask neurons to observe effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~12/04/2017\n",
    "### One-month paper proposal\n",
    "\n",
    "Micro-features are fine-grained acoustic features in speech. They can be used to profile many human aspects, yet they are difficult to measure. Identifying the formants and harmonics in speech is the first step of analyzing and evaluating the micro-features in speech. Many qualitative and quantitative measures, such as harmonic bandwidth and harmonic-to-noise ratio, require correct identification of harmonics. For a given utterance, identifying the harmonics require identifying the (frequency & bandwidth, duration & start/stop, amplitude & relative energy): $(f, \\Delta f, \\Delta t, t_0, t_1, A, E)$.\n",
    "\n",
    "\n",
    "Lets first review the conventional speech modeling. The MFCC feature sequences describe how resonant patterns vary with time. Then GMM models the collection of features as deviation from *universal background means*. With MAP mean-adaptation the speaker-dependent mean supervectors factorize into universal background mean supervectors plus total variations, yielding the toal variation subspace representation i-vectors. To summarize, the feature vectors are modeled by speaker-dependent normals.\n",
    "\n",
    "**Model**\n",
    "\n",
    "We can adopt similar subspace factorization directly in (frequency, time) domain\n",
    "\n",
    "$$y(\\omega, t) = L(\\omega, k) H(k, t) + n(\\omega, t)$$\n",
    "\n",
    "where $y(\\omega, t)$ is the signal amplitude at frequency $\\omega$ and time $t$, $H(k, t)$ represents the amplitude of harmonic $k$ at time $t$, and $n(\\omega, t)$ represents the noise at frequency $\\omega$ and time $t$. The $L(\\omega, k)$ is a factor matrix connecting $(\\omega, t)$ to $(k, t)$. Putting into vector form by discretizing over $F$ frequencies, $K$ harmonics at time $t$ yields\n",
    "\n",
    "$$y(t) = L H(t) + n(t)$$\n",
    "\n",
    "where $y(t) \\in \\mathbb{R}^{F}$, $L \\in \\mathbb{R}^{F \\times K}$, $H(t) \\in \\mathbb{R}^{K}$, and $n(t) \\in \\mathbb{R}^{F}$. Note that $H(t)$ is sparse. Our objective is\n",
    "\n",
    "$$\\text{arg}\\min_{H_t}\\; \\|H_t\\|_0 \\\\ \\text{s.t.}\\; \\|y_t - LH_t\\|_2 \\leq \\epsilon$$\n",
    "\n",
    "Given a collection of $y_t$, assume $n_t \\sim \\mathcal{N}(\\mu, \\sigma)$ and finite $\\{k_1, \\ldots, k_K\\}$, we need to figure out of way to solve the above problem.\n",
    "\n",
    "---\n",
    "A vocal register is a range of tones in the human voice produced by a particular vibratory pattern of the vocal folds. These registers include modal voice (or normal voice), vocal fry, falsetto, and the whistle register.\n",
    "\n",
    "### To-do\n",
    "Figure out of way to solve the above problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
